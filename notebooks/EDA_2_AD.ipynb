{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68416bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b96fa2",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a56d4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Users/andrei/code/images/images/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7a27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"/Users/andrei/code/images/images/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954cd3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "X_train = []\n",
    "for folder_path in os.listdir(train_path):\n",
    "    if not folder_path.startswith(\".\"):\n",
    "        for image_path in os.listdir(os.path.join(train_path, folder_path)):\n",
    "            img = load_img(os.path.join(os.path.join(train_path, folder_path), image_path), color_mode = \"grayscale\")\n",
    "            X_train.append(img_to_array(img))\n",
    "            y_train.append(os.path.basename(os.path.normpath(folder_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c960a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28ed0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "X_test = []\n",
    "for folder_path in os.listdir(test_path):\n",
    "    if not folder_path.startswith(\".\"):\n",
    "        for image_path in os.listdir(os.path.join(test_path, folder_path)):\n",
    "            img = load_img(os.path.join(os.path.join(test_path, folder_path), image_path), color_mode = \"grayscale\")\n",
    "            X_test.append(img_to_array(img))\n",
    "            y_test.append(os.path.basename(os.path.normpath(folder_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4961917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fade6",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07a4a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label_dict = {\n",
    "    'angry':0,\n",
    "    'disgust':0,\n",
    "    'fear':0,\n",
    "    'happy':2,\n",
    "    'neutral':1,\n",
    "    'sad':0,\n",
    "    'surprise':0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9e53181",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(y_train)\n",
    "y_df_num = y_df.applymap(lambda x : y_label_dict[x])\n",
    "y_num = np.array(y_df_num)\n",
    "\n",
    "y_train_cat = to_categorical(y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7890bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(y_test)\n",
    "y_df_num = y_df.applymap(lambda x : y_label_dict[x])\n",
    "y_num = np.array(y_df_num)\n",
    "\n",
    "y_test_cat = to_categorical(y_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d763c",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a67adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = np.max(pd.DataFrame(y_num).value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd2b4058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5785711807362687"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4a9cf",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2788e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48b6dbf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 17:25:10.898692: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "# Notice this cool new layer that \"pipe\" your rescaling within the architecture\n",
    "model2.add(Rescaling(1./255, input_shape=(48, 48, 1)))\n",
    "\n",
    "# Lets add 3 convolution layers, with relatively large kernel size as our pictures are quite big too\n",
    "model2.add(layers.Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model2.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model2.add(layers.Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model2.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model2.add(layers.Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model2.add(layers.MaxPooling2D(1))\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(100, activation='relu'))\n",
    "model2.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6d8108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dab88ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.9084 - accuracy: 0.5867 - val_loss: 0.7885 - val_accuracy: 0.6618\n",
      "Epoch 2/1000\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.7432 - accuracy: 0.6827 - val_loss: 0.9289 - val_accuracy: 0.5736\n",
      "Epoch 3/1000\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.6919 - accuracy: 0.7026 - val_loss: 0.8350 - val_accuracy: 0.6286\n",
      "Epoch 4/1000\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.6609 - accuracy: 0.7209 - val_loss: 0.8217 - val_accuracy: 0.6101\n",
      "Epoch 5/1000\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.6407 - accuracy: 0.7307 - val_loss: 0.6718 - val_accuracy: 0.7216\n",
      "Epoch 6/1000\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.6175 - accuracy: 0.7423 - val_loss: 0.7310 - val_accuracy: 0.6822\n",
      "Epoch 7/1000\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.5998 - accuracy: 0.7490 - val_loss: 0.9808 - val_accuracy: 0.5400\n",
      "Epoch 8/1000\n",
      "721/721 [==============================] - 14s 20ms/step - loss: 0.5849 - accuracy: 0.7549 - val_loss: 0.8640 - val_accuracy: 0.6029\n",
      "Epoch 9/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.5675 - accuracy: 0.7661 - val_loss: 0.6096 - val_accuracy: 0.7389\n",
      "Epoch 10/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.5573 - accuracy: 0.7700 - val_loss: 0.7661 - val_accuracy: 0.6623\n",
      "Epoch 11/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.5445 - accuracy: 0.7764 - val_loss: 0.8775 - val_accuracy: 0.6026\n",
      "Epoch 12/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.5288 - accuracy: 0.7839 - val_loss: 0.8156 - val_accuracy: 0.6513\n",
      "Epoch 13/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.5201 - accuracy: 0.7907 - val_loss: 0.9019 - val_accuracy: 0.6073\n",
      "Epoch 14/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.5096 - accuracy: 0.7921 - val_loss: 0.8532 - val_accuracy: 0.6241\n",
      "Epoch 15/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.4993 - accuracy: 0.7959 - val_loss: 0.7694 - val_accuracy: 0.6744\n",
      "Epoch 16/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.4891 - accuracy: 0.8009 - val_loss: 0.7840 - val_accuracy: 0.6609\n",
      "Epoch 17/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.4811 - accuracy: 0.8048 - val_loss: 0.8417 - val_accuracy: 0.6099\n",
      "Epoch 18/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.4738 - accuracy: 0.8081 - val_loss: 0.8332 - val_accuracy: 0.6456\n",
      "Epoch 19/1000\n",
      "721/721 [==============================] - 15s 20ms/step - loss: 0.4658 - accuracy: 0.8124 - val_loss: 0.9967 - val_accuracy: 0.6029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x161312a60>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_cat, batch_size = 32, epochs = 1000,\n",
    "          callbacks=[EarlyStopping(patience = 10, restore_best_weights= True, monitor = \"val_accuracy\", mode = \"max\")],\n",
    "         validation_split = 0.2, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b088cc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.7164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6728672981262207, 0.7163883447647095]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725462b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
